{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Among all of the business domains, HR is still the least disrupted. However, the latest developments in data collection and analysis tools and technologies allow for data driven decision-making in all dimensions, including HR. This course will provide a solid basis for dealing with employee data and developing a predictive model to analyze employee turnover."
      ],
      "metadata": {
        "id": "HyEDmjIObE8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUxQ_OS5YZkf",
        "outputId": "b27c7a9a-4f66-4b2c-840d-1eb760be2735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   satisfaction  evaluation  number_of_projects  average_montly_hours  \\\n",
            "0          0.38        0.53                   2                   157   \n",
            "1          0.80        0.86                   5                   262   \n",
            "2          0.11        0.88                   7                   272   \n",
            "3          0.72        0.87                   5                   223   \n",
            "4          0.37        0.52                   2                   159   \n",
            "\n",
            "   time_spend_company  work_accident  churn  promotion department  salary  \n",
            "0                   3              0      1          0      sales     low  \n",
            "1                   6              0      1          0      sales  medium  \n",
            "2                   4              0      1          0      sales  medium  \n",
            "3                   5              0      1          0      sales     low  \n",
            "4                   3              0      1          0      sales     low  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14999 entries, 0 to 14998\n",
            "Data columns (total 10 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   satisfaction          14999 non-null  float64\n",
            " 1   evaluation            14999 non-null  float64\n",
            " 2   number_of_projects    14999 non-null  int64  \n",
            " 3   average_montly_hours  14999 non-null  int64  \n",
            " 4   time_spend_company    14999 non-null  int64  \n",
            " 5   work_accident         14999 non-null  int64  \n",
            " 6   churn                 14999 non-null  int64  \n",
            " 7   promotion             14999 non-null  int64  \n",
            " 8   department            14999 non-null  object \n",
            " 9   salary                14999 non-null  object \n",
            "dtypes: float64(2), int64(6), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ],
      "source": [
        "# Import pandas (as pd) to read the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read \"turnover.csv\" and save it in a DataFrame called data\n",
        "data = pd.read_csv(\"turnover_data.csv\")\n",
        "\n",
        "# Take a quick look to the first 5 rows of data\n",
        "print(data.head())\n",
        "\n",
        "# Get some information on the types of variables in data\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the unique values of the \"department\" column\n",
        "print(data[\"department\"].unique())\n",
        "\n",
        "# Print the unique values of the \"salary\" column\n",
        "print(data[\"salary\"].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttthIkcibQJV",
        "outputId": "a8e62ce8-7486-47ef-83b2-29cc14030f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
            " 'product_mng' 'marketing' 'RandD']\n",
            "['low' 'medium' 'high']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing data for machine learning"
      ],
      "metadata": {
        "id": "RVx973nQcMmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the type of the \"salary\" column to categorical\n",
        "\n",
        "data[\"salary\"] = data[\"salary\"].astype('category')\n",
        "\n",
        "# Provide the correct order of categories\n",
        "data[\"salary\"] = data[\"salary\"].cat.reorder_categories([\"low\", \"medium\", \"high\"])\n",
        "\n",
        "# Encode categories\n",
        "data[\"salary\"] = data[\"salary\"].cat.codes"
      ],
      "metadata": {
        "id": "dJHbIBU4bkvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dummies and save them inside a new DataFrame\n",
        "departments = pd.get_dummies(data.department)\n",
        "\n",
        "# Take a quick look to the first 5 rows of the new DataFrame called departments\n",
        "print(departments.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5dGc0wUb_TQ",
        "outputId": "9cc544bd-1ecc-49a8-94f3-8302836022c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   IT  RandD  accounting  hr  management  marketing  product_mng  sales  \\\n",
            "0   0      0           0   0           0          0            0      1   \n",
            "1   0      0           0   0           0          0            0      1   \n",
            "2   0      0           0   0           0          0            0      1   \n",
            "3   0      0           0   0           0          0            0      1   \n",
            "4   0      0           0   0           0          0            0      1   \n",
            "\n",
            "   support  technical  \n",
            "0        0          0  \n",
            "1        0          0  \n",
            "2        0          0  \n",
            "3        0          0  \n",
            "4        0          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the \"accounting\" column to avoid \"dummy trap\"\n",
        "departments = departments.drop(\"accounting\", axis=1)\n",
        "\n",
        "# Drop the old column \"department\" as you don't need it anymore\n",
        "data = data.drop(\"department\", axis=1)\n",
        "\n",
        "# Join the new DataFrame \"departments\" to your employee dataset: done\n",
        "data = data.join(departments)"
      ],
      "metadata": {
        "id": "zJRm8xfScHFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Descriptive statistics"
      ],
      "metadata": {
        "id": "MS778hMmcWiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use len() function to get the total number of observations and save it as the number of employees\n",
        "n_employees = len(data)\n",
        "\n",
        "# Print the number of employees who left/stayed\n",
        "print(data[\"churn\"].value_counts())\n",
        "\n",
        "# Print the percentage of employees who left/stayed\n",
        "print(data[\"churn\"].value_counts()/n_employees*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzpvjoGHcZJk",
        "outputId": "f98b0722-56c3-4640-e6ea-9b14fabaec67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    11428\n",
            "1     3571\n",
            "Name: churn, dtype: int64\n",
            "0    76.191746\n",
            "1    23.808254\n",
            "Name: churn, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the dependent variable column (churn) and set it as target\n",
        "y = data[\"churn\"]\n",
        "\n",
        "# Drop column churn and set everything else as features\n",
        "X = data.drop(\"churn\",axis=1)"
      ],
      "metadata": {
        "id": "f3WL2p0edQGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function for splitting dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use that function to create the splits both for target and for features\n",
        "# Set the test sample to be 25% of your observations\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
      ],
      "metadata": {
        "id": "CLYQaArAdRoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Gini index\n",
        "##### The decision tree algorithm aims to achieve partitions in the terminal nodes that are as pure as possible. The Gini index is one of the methods used to achieve this. It is calculated based on the proportion of samples in each group. Given the number of people who stayed and left respectively, calculate the Gini index for that node."
      ],
      "metadata": {
        "id": "oV4_0GGXdpPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of people who stayed/left\n",
        "stayed = 37\n",
        "left = 1138\n",
        "\n",
        "#sum of stayed and left\n",
        "total = stayed + left\n",
        "\n",
        "#gini index\n",
        "gini = 2*(stayed/total)*(left/total)"
      ],
      "metadata": {
        "id": "lW7VAidIdq7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Given the Gini index that would result from splitting by either variable A or B, respectively, decide by which variable the tree should split next."
      ],
      "metadata": {
        "id": "gZCMUIXzeJMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gini index in case of splitting by variable A or B\n",
        "gini_A = 0.65\n",
        "gini_B = 0.15\n",
        "\n",
        "# check which Gini is lower and use it for spliting\n",
        "if gini_A < gini_B:\n",
        "    print(\"split by A!\")\n",
        "else:\n",
        "    print(\"split by B!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePD5WffkeFNH",
        "outputId": "63d60ae9-6335-42e4-ea19-b38d5b145daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split by B!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting the tree to employee data\n",
        "##### A train/test split provides the opportunity to develop the classifier on the training component and test it on the rest of the dataset. we will start developing an employee turnover prediction model using the decision tree classification algorithm. The algorithm provides a .fit() method, which can be used to fit the features to the model in the training set."
      ],
      "metadata": {
        "id": "9i7o7SQ3eU6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize it and call model by specifying the random_state parameter\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Apply a decision tree model to fit features to the target\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Check the accuracy score of the prediction for the training set\n",
        "print(model.score(X_train, y_train)*100)\n",
        "\n",
        "# Check the accuracy score of the prediction for the test set\n",
        "print(model.score(X_test, y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OZO_qFnec_E",
        "outputId": "df0955aa-4cf7-4ed7-9cae-b9970ac8519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n",
            "97.22666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the graphical visualization export function\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Apply Decision Tree model to fit Features to the Target\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to a dot file\n",
        "export_graphviz(model,\"tree.dot\")"
      ],
      "metadata": {
        "id": "6m0_ZqlefkEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pruning the tree\n",
        "##### Overfitting is a classic problem in analytics, especially for the decision tree algorithm. Once the tree is fully grown, it may provide highly accurate predictions for the training sample, yet fail to be that accurate on the test set. For that reason, the growth of the decision tree is usually controlled by:\n",
        "\n",
        "##### “Pruning” the tree and setting a limit on the maximum depth it can have. Limiting the minimum number of observations in one leaf of the tree.\n",
        "##### i will try to prune the tree and limit the growth of the tree to 5 levels of depthfit it to the employee data test prediction results on both training and testing sets."
      ],
      "metadata": {
        "id": "HaTbWDIdg1tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5\n",
        "model_depth_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model_depth_5.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy of the prediction for the training set\n",
        "print(model_depth_5.score(X_train, y_train)*100)\n",
        "\n",
        "# Print the accuracy of the prediction for the test set\n",
        "print(model_depth_5.score(X_test, y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37fbsX_hCPF",
        "outputId": "88039f4a-dfc5-422d-a115-774d2a59d7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.71535247577563\n",
            "97.06666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Limiting the sample size\n",
        "##### Another method to prevent overfitting is to specify the minimum number of observations necessary to grow a leaf (or node), in the Decision Tree.\n",
        "\n",
        "##### In this step, i will:\n",
        "##### set this minimum limit to 100 fit the new model to the employee data examine prediction results on both training and test sets"
      ],
      "metadata": {
        "id": "l3vbzLPlhSR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DecisionTreeClassifier while limiting the sample size in leaves to 100\n",
        "model_sample_100 = DecisionTreeClassifier(min_samples_leaf=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model_sample_100.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy of the prediction (in percentage points) for the training set\n",
        "print(model_sample_100.score(X_train, y_train)*100)\n",
        "\n",
        "# Print the accuracy of the prediction (in percentage points) for the test set\n",
        "print(model_sample_100.score(X_test, y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huNlvm3thcdZ",
        "outputId": "4420cd03-b616-4b95-e3b0-39dffe8edeb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.57747355320473\n",
            "96.13333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating accuracy metrics: precision\n",
        "##### The Precision score is an important metric used to measure the accuracy of a classification algorithm. It is calculated as the fraction of True Positives over the sum of True Positives and False Positives, or\n",
        " \n",
        "##### we define True Positives as the number of employees who actually left, and were classified correctly as leaving we define False Positives as the number of employees who actually stayed, but were wrongly classified as leaving If there are no False Positives, the precision score is equal to 1. If there are no True Positives, the precision score is equal to 0.\n",
        "\n",
        "##### I will calculate the precision score (using the sklearn function precision_score) for our initial classification model."
      ],
      "metadata": {
        "id": "Ns8YQYrlhwE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function to calculate precision score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# Predict whether employees will churn using the test set\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "# Calculate precision score by comparing target_test with the prediction\n",
        "precision_score(y_test, prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7LxOp3Dh5UA",
        "outputId": "fdbacf9c-82a5-4031-8d59-c3ffb14e7997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9240641711229947"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating accuracy metrics: recall\n",
        "##### The Recall score is another important metric used to measure the accuracy of a classification algorithm. It is calculated as the** fraction of True Positives over the sum of True Positives and False Negatives**, or If there are no False Negatives, the recall score is equal to 1. If there are no True Positives, the recall score is equal to 0.\n",
        "\n",
        "##### In this step, i will calculate the recall score (using the sklearn function recall_score) for your initial classification model."
      ],
      "metadata": {
        "id": "fKf3Ng4piir4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function to calculate recall score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Use the initial model to predict churn\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "# Calculate recall score by comparing target_test with the prediction\n",
        "recall_score(y_test, prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP3EShFNir7p",
        "outputId": "06f584f6-d06a-4796-9912-9e1534a8ffb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9632107023411371"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating the ROC/AUC score\n",
        "##### While the Recall score is an important metric for measuring the accuracy of a classification algorithm, it puts too much weight on the number of False Negatives. On the other hand, Precision is concentrated on the number of False Positives. The combination of those two results in the ROC curve allows us to measure both recall and precision. The area under the ROC curve is calculated as the AUC score.\n",
        "\n",
        "##### I will calculate the ROC/AUC score for the initial model using the sklearn roc_auc_score() function."
      ],
      "metadata": {
        "id": "ViA4Kt2Gi2QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function to calculate ROC/AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Use initial model to predict churn (based on features_test)\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "# Calculate ROC/AUC score by comparing target_test with the prediction\n",
        "roc_auc_score(y_test, prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFGF8-Loi_YR",
        "outputId": "ac5e6830-8f6c-4df7-91da-f4f6998d0b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9691623087590718"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Balancing classes\n",
        "##### It can significantly affect prediction results, as shown by the difference between the recall and accuracy scores. To solve the imbalance, equal weights are usually given to each class. Using the class_weight argument in sklearn's DecisionTreeClassifier, one can make the classes become \"balanced\".\n",
        "\n",
        "##### Let’s correct our model by solving its imbalance problem: first, i'm going to set up a model with balanced classes then, you will fit it to the training data finally, you will check its accuracy on the test set"
      ],
      "metadata": {
        "id": "sK2HzQ5VjXMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DecisionTreeClassifier \n",
        "model_depth_5_b = DecisionTreeClassifier(max_depth=5,class_weight=\"balanced\",random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model_depth_5_b.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy of the prediction (in percentage points) for the test set\n",
        "print(model_depth_5_b.score(X_test, y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyLcmUwtjiXr",
        "outputId": "d90b95bf-de9e-4788-ba4e-dee4983826a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93.70666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison of Employee attrition models\n",
        "##### my task here is to compare the balanced and imbalanced (default) models using the pruned tree (max_depth=7). The imbalanced model is already done using recall and ROC/AUC scores. Complete the same steps for the balanced model.\n",
        "##### The variables X_train, y_train, X_test and y_test are already available in my workspace. An imbalanced model has already been fit for you and, and its predictions saved as prediction.\n",
        "##### The functions recall_score() and roc_auc_score() have been imported for you."
      ],
      "metadata": {
        "id": "CeJr0JEVkVJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the recall score\n",
        "print(recall_score(y_test,prediction))\n",
        "# Print the ROC/AUC score\n",
        "print(roc_auc_score(y_test,prediction))\n",
        "\n",
        "# Initialize the model\n",
        "model_depth_7_b = DecisionTreeClassifier(max_depth=7,class_weight=\"balanced\",random_state=42)\n",
        "\n",
        "# Fit it to the training component\n",
        "model_depth_7_b.fit(X_train, y_train)\n",
        "# Make prediction using test component\n",
        "prediction_b = model_depth_7_b.predict(X_test)\n",
        "# Print the recall score for the balanced model\n",
        "print(recall_score(y_test, prediction_b))\n",
        "# Print the ROC/AUC score for the balanced model\n",
        "print(roc_auc_score(y_test, prediction_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCPJFwi-kiZ2",
        "outputId": "5c658687-36d5-4e37-fa9e-77bcb7ff9cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9632107023411371\n",
            "0.9691623087590718\n",
            "0.9319955406911928\n",
            "0.959863876199084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-validation using sklearn:\n",
        "##### overfitting the dataset is a common problem in analytics. This happens when a model has learned the data too closely: it has great performances on the dataset it was trained on, but fails to generalize outside of it. \n",
        "##### While the train/test split technique ensures that the model does not overfit the training set, hyperparameter tuning may result in overfitting the test component, since it consists in tuning the model to get the best prediction results on the test set. Therefore, it is recommended to validate the model on different testing sets. K-fold cross-validation allows us to achieve this:\n",
        "##### it splits the dataset into a training set and a testing set it fits the model, makes predictions and calculates a score (you can specify if you want the accuracy, precision, recall…) it repeats the process k times in total it outputs the average of the 10 scores."
      ],
      "metadata": {
        "id": "fJ2LaaF8ldJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function for implementing cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use that function to print the cross validation score for 10 folds\n",
        "print(cross_val_score(model, X, y,cv=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11XjZRcmeAS",
        "outputId": "1accd1e7-d781-4849-9cf1-cc31f5524193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98533333 0.98533333 0.974      0.96533333 0.96       0.97933333\n",
            " 0.99       0.99333333 1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up GridSearch parameters\n",
        "##### A hyperparameter is a parameter inside a function. For example, max_depth or min_samples_leaf are hyperparameters of the DecisionTreeClassifier() function. Hyperparameter tuning is the process of testing different values of hyperparameters to find the optimal ones: the one that gives the best predictions according to your objectives. In sklearn, you can use GridSearch to test different combinations of hyperparameters. Even better, you can use GridSearchCV() to test different combinations and run cross-validation on them in one function!\n",
        "\n",
        "##### I'm going to prepare the different values you want to test for max_depth and min_samples_leaf. You will then put these in a dictionary, because that’s what is required for GridSearchCV():\n",
        "\n",
        "##### the dictionary keys will be the hyperparameters names\n",
        "##### the dictionary values will be the attributes (the hyperparameter values) you want to test Instead of writing all the values manually, you will use the range() function, which allows us to generate values incrementally. For example, range(1, 10, 2) will generate a list containing values ranging from 1 included to 10 not included, by increments of 2. So the final result will be [1, 3, 5, 7, 9]."
      ],
      "metadata": {
        "id": "ARoIUO3UmqJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate values for maximum depth\n",
        "depth = [i for i in range(5,21,1)]\n",
        "\n",
        "# Generate values for minimum sample size\n",
        "samples = [i for i in range(50,500,50)]\n",
        "\n",
        "# Create the dictionary with parameters to be checked\n",
        "parameters = dict(max_depth=depth, min_samples_leaf=samples)"
      ],
      "metadata": {
        "id": "Tcg-OZ0Em2Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing GridSearch"
      ],
      "metadata": {
        "id": "9hhD2gy4nAMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the GridSearchCV function\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# set up parameters: done\n",
        "parameters = dict(max_depth=depth, min_samples_leaf=samples)\n",
        "\n",
        "# initialize the param_search function using the GridSearchCV function, initial model and parameters above\n",
        "param_search = GridSearchCV(model, parameters, cv=3)\n",
        "\n",
        "# fit the param_search to the training dataset\n",
        "param_search.fit(X_train, y_train)\n",
        "\n",
        "# print the best parameters found\n",
        "print(param_search.best_params_)\n",
        "\n",
        "# obtaining the best model\n",
        "\n",
        "best_model = param_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EhDLUlwnBAC",
        "outputId": "92a2d093-f46f-428d-c561-2f67dd1ac041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 5, 'min_samples_leaf': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sorting important features\n",
        "##### Among other things, Decision Trees are very popular because of their interpretability. Many models can provide accurate predictions, but Decision Trees can also quantify the effect of the different features on the target. Here, it can tell you which features have the strongest and weakest impacts on the decision to leave the company. In sklearn, you can get this information by using the feature_importances_ attribute.\n",
        "##### I'm going to get the quantified importance of each feature, save them in a pandas DataFrame (a Pythonic table), and sort them from the most important to the less important."
      ],
      "metadata": {
        "id": "wq8BzEfEnoVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate feature importances\n",
        "feature_importances = best_model.feature_importances_\n",
        "\n",
        "# Create a list of features: done\n",
        "feature_list = list(X)\n",
        "\n",
        "# Save the results inside a DataFrame using feature_list as an index\n",
        "relative_importances = pd.DataFrame(index=feature_list, data=feature_importances, columns=[\"importance\"])\n",
        "\n",
        "# Sort values to learn most important features\n",
        "relative_importances.sort_values(by=\"importance\", ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "1MN6i1QEnzJg",
        "outputId": "f1f96cae-ffeb-44b5-b27d-de74de5a99cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      importance\n",
              "satisfaction            0.551529\n",
              "time_spend_company      0.157009\n",
              "evaluation              0.144354\n",
              "number_of_projects      0.092864\n",
              "average_montly_hours    0.053087\n",
              "technical               0.000631\n",
              "hr                      0.000295\n",
              "salary                  0.000231\n",
              "promotion               0.000000\n",
              "work_accident           0.000000\n",
              "RandD                   0.000000\n",
              "management              0.000000\n",
              "marketing               0.000000\n",
              "product_mng             0.000000\n",
              "sales                   0.000000\n",
              "support                 0.000000\n",
              "IT                      0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a10c3bd-123f-432e-9c73-3c5cdcd8ba3b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>satisfaction</th>\n",
              "      <td>0.551529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time_spend_company</th>\n",
              "      <td>0.157009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evaluation</th>\n",
              "      <td>0.144354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_projects</th>\n",
              "      <td>0.092864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_montly_hours</th>\n",
              "      <td>0.053087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>technical</th>\n",
              "      <td>0.000631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hr</th>\n",
              "      <td>0.000295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>salary</th>\n",
              "      <td>0.000231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>promotion</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>work_accident</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandD</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>management</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>marketing</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_mng</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sales</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IT</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a10c3bd-123f-432e-9c73-3c5cdcd8ba3b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a10c3bd-123f-432e-9c73-3c5cdcd8ba3b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a10c3bd-123f-432e-9c73-3c5cdcd8ba3b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selecting important features\n",
        "##### In this exercise, your task is to select only the most important features that will be used by the final model. Remember, that the relative importances are saved in the column importance of the DataFrame called relative_importances."
      ],
      "metadata": {
        "id": "qRXUAWoRovRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select only features with relative importance higher than 1%\n",
        "selected_features = relative_importances[relative_importances.importance>0.01]\n",
        "\n",
        "# create a list from those features: done\n",
        "selected_list = selected_features.index\n",
        "\n",
        "# transform both features_train and features_test components to include only selected features\n",
        "features_train_selected = X_train[selected_list]\n",
        "features_test_selected = X_test[selected_list]"
      ],
      "metadata": {
        "id": "pGh-XB5bozOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Develop and test the best model\n",
        "##### I found out that the following parameters allow me to get better model:\n",
        "##### max_depth = 8,\n",
        "##### min_samples_leaf = 150,\n",
        "##### class_weight = \"balanced\"\n",
        "##### I discovered that some of the features have a negligible impact. I realized that i could get accurate predictions using just a small number of selected, impactful features and i updated my training and testing set accordingly, creating the variables **X_train_selected** and **X_test_selected**.\n",
        "##### With all this information at my disposal, i'm now going to develop the best model for predicting employee turnover and evaluate it using the appropriate metrics."
      ],
      "metadata": {
        "id": "DfhHFK9qpCnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected = data[[\"satisfaction\", \"evaluation\", \"number_of_projects\" ,\"average_montly_hours\", \"time_spend_company\"]]\n",
        "y_selected = data[\"churn\"]"
      ],
      "metadata": {
        "id": "r1H4sQfkpvNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_selected, X_test_selected, y_train, y_test = train_test_split(X_selected, y_selected, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "RYugRGTmqVLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the best model using parameters provided in description\n",
        "model_best = DecisionTreeClassifier(max_depth=8, min_samples_leaf=150, class_weight=\"balanced\", random_state=42)\n",
        "\n",
        "# Fit the model using only selected features from training set: done\n",
        "model_best.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make prediction based on selected list of features from test set\n",
        "prediction_best = model_best.predict(X_test_selected)\n",
        "\n",
        "# Print the general accuracy of the model_best\n",
        "print(model_best.score(X_test_selected, y_test) * 100)\n",
        "\n",
        "# Print the recall score of the model predictions\n",
        "print(recall_score(y_test, prediction_best) * 100)\n",
        "\n",
        "# Print the ROC/AUC score of the model predictions\n",
        "print(roc_auc_score(y_test, prediction_best) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63r35E3GpZ73",
        "outputId": "ef447b8a-feb3-4465-c8a1-d7769a94bf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.28\n",
            "91.75027870680044\n",
            "94.07002193314084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree, export_text\n",
        "plot_tree(model_best, feature_names=X_selected, class_names=y, filled=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "eFznkSC2uneL",
        "outputId": "10078965-03e4-4ff8-a99c-95e923790337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2f6235138bc0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mplot_tree\u001b[0;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mexporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, decision_tree, ax)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mmy_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mdraw_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuchheim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36m_make_tree\u001b[0;34m(self, node_id, et, criterion, depth)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;31m# traverses _tree.Tree recursively, builds intermediate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# \"_reingold_tilford.Tree\" object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         if et.children_left[node_id] != _tree.TREE_LEAF and (\n\u001b[1;32m    633\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mnode_to_str\u001b[0;34m(self, tree, node_id, criterion)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# Always write node decision criteria, except for leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 feature = \"x%s%s%s\" % (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}